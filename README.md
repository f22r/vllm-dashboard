# vLLM Dashboard

Real-time monitoring and management dashboard for vLLM server with system resource visualization, model management, and integrated chat.

![Dashboard Preview](docs/preview.png)

## âœ¨ Features

### ğŸ–¥ï¸ Dashboard & Monitoring
- **Real-time Monitoring** - WebSocket-based live updates every 1.5 seconds.
- **System Resources**:
  - **CPU**: Usage percentage and temperature.
  - **Memory**: RAM utilization.
  - **GPU**: NVIDIA GPU stats including VRAM usage, Total VRAM, Compute utilization, Power, and Temperature.
  - **Disk**: Overall disk usage monitoring.
  - **Network**: Real-time upload/download rates.
- **vLLM Status**: Connection status connection to the backend.

### ğŸ“¦ Model Management
- **Process Control**: 
  - Start and Stop specific vLLM models.
  - **Zombie Killer**: Detects and terminates orphaned "zombie" vLLM processes.
  - **Kill All**: One-click emergency stop to terminate all model processes.
- **Model Library**: 
  - View available models in your local HuggingFace cache.
  - **Download Manager**: Download new models directly from HuggingFace with progress tracking.
- **Multi-Model Support**: Run different models on different ports (managed automatically).

### ğŸ’¬ Chat Interface
- **Interactive Chat**: Direct interface to chat with your running vLLM models.
- **Auto-Configuration**: Automatically connects to the specific port of the selected running model.
- **Template Handling**: Includes generic fallback templates for models that don't ship with valid chat templates (e.g., base OPT/Pythia models).

### ğŸ¨ Modern UI
- **Design**: Premium dark theme with glassmorphism aesthetics.
- **Tech**: Built with React (Vite) and generic CSS (PostCSS/Tailwind-ready).

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Node.js 20+
- NVIDIA GPU (Recommended for vLLM)
- Linux OS

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd vllm-dashboard

# 1. Setup Backend
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 2. Setup Frontend
cd ../frontend
npm install
```

### Running the Application

You need two terminal windows:

**Terminal 1 - Backend:**
```bash
cd backend
source venv/bin/activate
# Uses port from .env (default: 5511)
python main.py
```

**Terminal 2 - Frontend:**
```bash
cd frontend
# Uses port from .env (default: 5510)
npm run dev
```

Open your browser at **http://localhost:5510**

## ğŸ“ Project Structure

```
vllm-dashboard/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py           # FastAPI app, WebSocket, & Process Manager
â”‚   â”œâ”€â”€ monitoring.py     # System stats (psutil, pynvml)
â”‚   â”œâ”€â”€ vllm_service.py   # vLLM Client integration
â”‚   â”œâ”€â”€ chat_template.jinja # Fallback Jinja2 template
â”‚   â””â”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.jsx      # Main resource monitor
â”‚   â”‚   â”‚   â”œâ”€â”€ ModelManager.jsx   # Model start/stop/download
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.jsx  # Chat UI
â”‚   â”‚   â”œâ”€â”€ components/   # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ App.jsx       # Routing & Layout
â”‚   â”‚   â””â”€â”€ main.js       # Entry point
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

The application uses environment variables for configuration. You can create a `.env` file in the project root.

| Variable | Default | Description |
|----------|---------|-------------|
| `VLLM_URL` | `http://localhost:8001` | Default vLLM URL |
| `FRONTEND_PORT` | `5510` | Frontend dev server port |
| `BACKEND_PORT` | `5511` | Backend API port |
| `HF_TOKEN` | *None* | Hugging Face Token (Required for gated models) |


## ğŸ”Œ API Endpoints

### Process Management
| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/vllm/start` | POST | Start a generic model instance |
| `/api/vllm/stop` | POST | Stop a model or all models |
| `/api/vllm/control/status` | GET | Get list of managed processes (running/zombies) |
| `/api/vllm/download` | POST | Trigger a HuggingFace model download |
| `/api/vllm/available-models` | GET | List models in local HF cache |

### Chat & Inference
| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/vllm/chat` | POST | Proxy chat request to specific model port |

### System Monitoring
| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/system/all` | GET | Complete system stats snapshot |
| `/ws/monitoring` | WebSocket | Real-time stream of System + vLLM + Process data |

## ğŸ“„ License

MIT License

---

Made with â¤ï¸ for the Local AI Community
